#!/bin/bash
# Automated script to update dashboard with fresh data

# Set working directory
cd /home/bam/data_science_project

# Log start time
echo "$(date): Starting dashboard update process" >> logs/dashboard_updates.log

# Check if there are new processed data files to commit
if [ -n "$(git status --porcelain data/processed/)" ]; then
    echo "$(date): New processed data found, updating dashboard..." >> logs/dashboard_updates.log
    
    # Add new processed data files
    git add data/processed/*.csv
    
    # Create commit with timestamp
    git commit -m "Automated data update: $(date '+%Y-%m-%d %H:%M:%S')

- Updated processed data from daily pipeline run
- Quality score: $(python -c "import pandas as pd; df=pd.read_csv('data/processed/processed_data_$(date +%Y%m%d)*.csv'); print('Latest data available')" 2>/dev/null || echo 'Data updated')
- Automated by cron job

ðŸ¤– Generated by automated pipeline"
    
    # Push to GitHub (triggers Streamlit Cloud update)
    if git push origin main; then
        echo "$(date): Successfully pushed updates to GitHub" >> logs/dashboard_updates.log
        echo "$(date): Streamlit Cloud will auto-deploy in 2-3 minutes" >> logs/dashboard_updates.log
    else
        echo "$(date): ERROR - Failed to push to GitHub" >> logs/dashboard_updates.log
        exit 1
    fi
else
    echo "$(date): No new data to update" >> logs/dashboard_updates.log
fi

echo "$(date): Dashboard update process completed" >> logs/dashboard_updates.log